Mocha的分库分表（为何这样设计，以后怎么数据迁移，中间件），缓存一致性，用户关注动态的方案（读散或者写扩散），MQ消费顺序问题，缓存的几种模式，如何初始化数据。扩

数据聚合服务：分布式JOB，定制化查询，ES的部署，分片，优化，原理，脑裂。
如何做到强一致性？

SSO：扫码登录，长连接问题

Spring Cloud 服务的注册和发现，链路追踪，Robbin，分布式事务，分布式锁，Sentinel，Nacos，seata

Redis实现的分布式锁能否锁得住
MySQL单机和集群情况下丢数据

Netty，JVM、JDK（集合，IO，多线程，并发包）


实现内存缓存；
多节点通知使用Redis的 publish/subscribe；
可能会有消息乱序的问题，但是每次只发送要更新的id，再用id从数据库里拿到最新的，这里即便乱序了数据也是一致的。
如果要强一致怎么实现。 如果更新时，事务还没提交好，发出去的消息被处理了，去数据库查询查询到旧值怎么办？（如何在事务提交后发消息 TransactionSynchronizationAdapter, TransactionSynchronizationManager）

BaiDu-Uid-Generator: 过长导致前端无法显示（JS只能处理53位的数值），增加sign的长度，压缩时间，workID、sequence的长度。
workID每次重启不一样，基于Eureka Client实现一个可回收的WorkIDAssigner。

MongoDB测试:
MongoDB支持集群分片模式，但是需要指定Master节点，指定分片键，Hashed索引效果很好，单条检索速度很快，批量写入速度蛮快，无索引查询难以接受，addToSet原子操作可以用来做蛮多分布式处理。写入大约8千万文档，Total Size In Memory：21.35 GB (22,923,608,254)，Average Of Objects Size： 285 bytes (285)，Storage Size：7.43 GB (7,981,064,192)，index Size： 3.87 GB (4,154,327,040)，单机配置需要提高。

MongoDB：3分片，每个分片额外2个机器做备份，共计9台机器。
采用 _id 作为分片键。 findAndModify 中的查询如果不能 _id = xxx，就会报 ShardKeyNotFound，需要调研下原因。

### 关于分片键的选择与分片方式的选择：
分片键要选择合适，能够覆盖核心查询的性能要求。
分片方式主要为：范围分片和Hash分片，如果是用自增ID作为分片键，用范围分片有一个问题是：如果最新的数据最常用成了热数据，那么会导致最新的数据只存在于某一个节点，进而导致这一个节点的压力最大。这时候使用Hash分片分布会均匀一些。

名单的快速匹配索引：一条名单要匹配，则能匹配的规则是请求传来的规则的非空子集。例如 传过来的是 k1=v1,k2=v2...kn=vn，那么，配置的规则要与这条匹配上，则只能配置成 k1=v1 或者 k2=v2 或者 k1=v1 and k2=v2，即是其非空子集即可。这样只需要将传入的规则排序后，取其各种组合算出 hash值即可，这样可以用hash值快速索引，比mongodb的数组索引性能好。

大数据过滤：RoaringBitMap

串线：Tomcat Request复用问题，多线程上下文传递问题，线程池
日志导致应用变慢的问题